{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35485f3-1b37-48ea-a28f-fe27b381d011",
   "metadata": {},
   "source": [
    "# NLP Application 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df0ad4c-e1c8-4085-bced-bbe8a90b980e",
   "metadata": {},
   "source": [
    "### This code builds and tests a text classification model on the restaurant reviews dataset. The dataset is cleaned and feature extraction is performed. The Naive Bayes model is trained and predictions are made on the test data. The complexity matrix is used to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e55297-48f8-4b9a-953d-62f425409ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78367c-f46a-45f5-be34-b4f415799ff1",
   "metadata": {},
   "source": [
    "### File path and data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9269881-e35f-4552-b7b4-eef989894499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Review  Liked\n",
      "0                            Wow... Loved this place.       1\n",
      "1                                  Crust is not good.       0\n",
      "2           Not tasty and the texture was just nasty.       0\n",
      "3    Stopped by during the late May bank holiday of...      1\n",
      "4    The selection on the menu was great and so wer...      1\n",
      "..                                                 ...    ...\n",
      "992  I think food should have flavor and texture an...      0\n",
      "993                          Appetite instantly gone.       0\n",
      "994  Overall I was not impressed and would not go b...      0\n",
      "995  The whole experience was underwhelming  and I ...      0\n",
      "996  Then  as if I hadn't wasted enough of my life ...      0\n",
      "\n",
      "[997 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "Reviews = pd.read_csv(r'C:\\Users\\Arif Furkan\\OneDrive\\Belgeler\\Python_kullanirken\\Restaurant_Reviews.csv')\n",
    "print(Reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602cd22-163c-4229-8323-159c8330032a",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7101db05-4f1f-4ae5-826b-2e64feb5a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Arif\n",
      "[nltk_data]     Furkan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "collection = []\n",
    "for i in range(len(Reviews)):\n",
    "    Comment = re.sub('[^a-zA-Z]',' ',Reviews['Review'][i])\n",
    "    Comment = Comment.lower()\n",
    "    Comment = Comment.split()\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    Comment = [ps.stem(Word) for Word in Comment if Word not in english_stopwords]\n",
    "    Comment = ' '.join(Comment)\n",
    "    collection.append(Comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a507cef-e8e9-4759-bc59-79e2416ad55a",
   "metadata": {},
   "source": [
    "### Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a94af14-d616-4894-bae8-0c823b51439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1996)\n",
    "X = cv.fit_transform(collection).toarray()\n",
    "y = Reviews.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634eb069-e159-43e9-949e-6c61b0e1cdf0",
   "metadata": {},
   "source": [
    "### Separation of Data Set into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e39418-4060-4222-adc2-5ecdc144e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 0 1\n",
      " 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 1\n",
      " 1 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1\n",
      " 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1\n",
      " 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1\n",
      " 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 0\n",
      " 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 0\n",
      " 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1\n",
      " 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 1\n",
      " 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0\n",
      " 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
      " 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1\n",
      " 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 1\n",
      " 1 0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1\n",
      " 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
      " 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1\n",
      " 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0\n",
      " 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e723f-dc3d-45e1-8476-9e6c604b9e95",
   "metadata": {},
   "source": [
    "### Naive Bayes Model Training and Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48bbbca-6054-4ddf-a684-2a1330714531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train,y_train)\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df177f3d-22e5-4293-a2b8-f1e643e6fc5d",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c940b991-81d5-477c-bb8b-0d77fbc85072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48 47]\n",
      " [18 87]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
